{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from pdf2image import convert_from_path\n",
    "from byaldi import RAGMultiModalModel\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda542a0b9f24670be20d3e38cc95754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float32\n",
    ").eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbosity is set to 1 (active). Pass verbose=0 to make quieter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe28e0af6954edeaf903cc677aac622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RAG = RAGMultiModalModel.from_pretrained(\"vidore/colpali\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, max_size=(800, 800)):\n",
    "    image.thumbnail(max_size, Image.Resampling.LANCZOS)  # Resize the image while maintaining aspect ratio\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(image):\n",
    "    # Prepare the image and text as inputs for Huggingface model\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image}\n",
    "        ]\n",
    "    }]\n",
    "    \n",
    "    # Prepare the text and image inputs for the model\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    \n",
    "    # Generate output from Huggingface model on CPU\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )  # No need for .to(\"cuda\"), it defaults to CPU\n",
    "\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "    generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "    extracted_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True)\n",
    "    \n",
    "    return extracted_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_keyword(extracted_text, search_query):\n",
    "    # Normalize both extracted text and search query\n",
    "    normalized_text = extracted_text.lower()\n",
    "    normalized_query = search_query.lower()\n",
    "\n",
    "    # Check for the presence of the normalized query in the normalized text\n",
    "    if normalized_query in normalized_text:\n",
    "        return f'Keyword \"{search_query}\" found in text!'\n",
    "\n",
    "    # If not found, provide feedback\n",
    "    return f'Keyword \"{search_query}\" not found in text.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_text(extracted_text):\n",
    "    # Ensure we split by paragraph rather than assuming a table format\n",
    "    structured_text = extracted_text.split('\\n\\n')  # Split by double newlines (paragraph breaks)\n",
    "    \n",
    "    # Prepare structured text output (no table assumption)\n",
    "    structured_output = '\\n\\n'.join([f'Paragraph {i+1}: {para.strip()}' for i, para in enumerate(structured_text) if para.strip()])\n",
    "    \n",
    "    # Convert structured text to JSON format (no table assumption)\n",
    "    json_output = {\"paragraphs\": [{\"id\": i+1, \"content\": para.strip()} for i, para in enumerate(structured_text) if para.strip()]}\n",
    "    \n",
    "    return structured_output, json_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_and_search(image, search_query):\n",
    "    # Resize the image before extracting text\n",
    "    resized_image = resize_image(image)\n",
    "    \n",
    "    # Extract text from the resized image\n",
    "    extracted_text = extract_text(resized_image)\n",
    "    \n",
    "    # Perform the search within the extracted text\n",
    "    search_result = search_keyword(extracted_text, search_query)\n",
    "    \n",
    "    # Structure the extracted text and convert to JSON format\n",
    "    structured_text, json_output = structure_text(extracted_text)\n",
    "    \n",
    "    return extracted_text, search_result, structured_text, json_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "interface = gr.Interface(\n",
    "    fn=ocr_and_search,\n",
    "    inputs=[\n",
    "        gr.Image(type=\"pil\", label=\"Upload Image (JPEG, PNG, etc.)\"),  # Allow users to upload an image in common formats\n",
    "        gr.Textbox(label=\"Enter keyword for search\")  # Input for search keyword\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Extracted Text\"),  # Display the extracted text\n",
    "        gr.Textbox(label=\"Search Result\"),  # Display the search result\n",
    "        gr.Textbox(label=\"Structured Text\"),  # Display structured extracted text\n",
    "        gr.JSON(label=\"JSON Output\")  # Display JSON format of the structured text\n",
    "    ],\n",
    "    title=\"OCR and Search with Image Resizing and Text Structuring\",\n",
    "    description=\"Upload an image containing Hindi/English text, resize it for optimal performance, extract the text, structure it, and convert to JSON format. You can also search for a keyword in the extracted text.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7866\n",
      "Running on public URL: https://4a7b64094ee545edb9.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://4a7b64094ee545edb9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interface.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30776,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
